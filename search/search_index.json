{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EESSI project documentation! \u00b6 Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Project partners Contact info The EESSI project was presented at the HPC Knowledge Meeting in June 2020. Check the recording:","title":"Home"},{"location":"#welcome-to-the-eessi-project-documentation","text":"Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Project partners Contact info The EESSI project was presented at the HPC Knowledge Meeting in June 2020. Check the recording:","title":"Welcome to the EESSI project documentation!"},{"location":"compatibility_layer/","text":"Compatibility layer \u00b6 The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"compatibility_layer/#compatibility-layer","text":"The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"contact/","text":"Contact info \u00b6 For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"contact/#contact-info","text":"For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"filesystem_layer/","text":"Filesystem layer \u00b6 The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Filesystem layer"},{"location":"filesystem_layer/#filesystem-layer","text":"The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Filesystem layer"},{"location":"overview/","text":"Overview of the EESSI project \u00b6 Scope & Goals \u00b6 Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently. Inspiration \u00b6 The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented . Layered structure \u00b6 The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on. Opportunities \u00b6 We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community. Challenges \u00b6 There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI; Current status \u00b6 (June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Project overview"},{"location":"overview/#overview-of-the-eessi-project","text":"","title":"Overview of the EESSI project"},{"location":"overview/#scope-goals","text":"Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently.","title":"Scope &amp; Goals"},{"location":"overview/#inspiration","text":"The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented .","title":"Inspiration"},{"location":"overview/#layered-structure","text":"The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on.","title":"Layered structure"},{"location":"overview/#opportunities","text":"We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community.","title":"Opportunities"},{"location":"overview/#challenges","text":"There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI;","title":"Challenges"},{"location":"overview/#current-status","text":"(June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Current status"},{"location":"partners/","text":"Project partners \u00b6 Delft University of Technology (The Netherlands) \u00b6 Robbert Eggermont Koen Mulderij Dell Technologies (Europe) \u00b6 Walther Blom, High Education & Research Jaco van Dijk, Higher Education Eindhoven University of Technology \u00b6 Patrick Van Brakel Ghent University (Belgium) \u00b6 Kenneth Hoste, HPC-UGent HPCNow! (Spain) \u00b6 Oriol Mula Valls J\u00fclich Supercomputing Centre (Germany) \u00b6 Alan O'Cais University of Cambridge (United Kingdom) \u00b6 Mark Sharpley, Research Computing Services Division University of Groningen (The Netherlands) \u00b6 Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology University of Twente (The Netherlands) \u00b6 Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS) University of Oslo (Norway) \u00b6 Thomas R\u00f6blitz Vrije Universiteit Amsterdam (The Netherlands) \u00b6 Peter Stol SURF (The Netherlands) \u00b6 Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"Project partners"},{"location":"partners/#project-partners","text":"","title":"Project partners"},{"location":"partners/#delft-university-of-technology-the-netherlands","text":"Robbert Eggermont Koen Mulderij","title":"Delft University of Technology (The Netherlands)"},{"location":"partners/#dell-technologies-europe","text":"Walther Blom, High Education & Research Jaco van Dijk, Higher Education","title":"Dell Technologies (Europe)"},{"location":"partners/#eindhoven-university-of-technology","text":"Patrick Van Brakel","title":"Eindhoven University of Technology"},{"location":"partners/#ghent-university-belgium","text":"Kenneth Hoste, HPC-UGent","title":"Ghent University (Belgium)"},{"location":"partners/#hpcnow-spain","text":"Oriol Mula Valls","title":"HPCNow! (Spain)"},{"location":"partners/#julich-supercomputing-centre-germany","text":"Alan O'Cais","title":"J\u00fclich Supercomputing Centre (Germany)"},{"location":"partners/#university-of-cambridge-united-kingdom","text":"Mark Sharpley, Research Computing Services Division","title":"University of Cambridge (United Kingdom)"},{"location":"partners/#university-of-groningen-the-netherlands","text":"Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology","title":"University of Groningen (The Netherlands)"},{"location":"partners/#university-of-twente-the-netherlands","text":"Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS)","title":"University of Twente (The Netherlands)"},{"location":"partners/#university-of-oslo-norway","text":"Thomas R\u00f6blitz","title":"University of Oslo (Norway)"},{"location":"partners/#vrije-universiteit-amsterdam-the-netherlands","text":"Peter Stol","title":"Vrije Universiteit Amsterdam (The Netherlands)"},{"location":"partners/#surf-the-netherlands","text":"Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"SURF (The Netherlands)"},{"location":"software_layer/","text":"Software layer \u00b6 The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"},{"location":"software_layer/#software-layer","text":"The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"}]}